{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Random Forest Classifiers to detect anomalies from FinBIF specimen data\n",
    "\n",
    "Random Forest is recommended method by many scientific papers. It only evaluates a random subset of predictors to identify the best predictors instead of searching over all predictors. \n",
    "\n",
    "Down-sampling (‘balanced RF') is used in this method to reduce the bias of overlapping / imbalanced classes. See https://nsojournals.onlinelibrary.wiley.com/doi/full/10.1111/ecog.05615"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import helpers\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore') # Suppress warnings for cleaner output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read the data. The geopackage has 289 000 YKJ squares (10 km x 10 km grid polygons). \n",
    "Each of them contain:\n",
    "    - the bird species in the square, and its atlas class value, ID and name\n",
    "    - CORINE land cover proportions\n",
    "    - temperature and height\n",
    "    - YKJ coordinates (ykj_n and ykj_e)\n",
    "    - the lenght of the coastline in the square\n",
    "\n",
    "In finland, there are 3816 YKJ squares in total, so this geopackage has multiple overlapping polygons, one for each bird.\n",
    "\"\"\"\n",
    "gdf = gpd.read_file('YKJ10km_polygons\\YKJ_Corine_birds2.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns (environment, habitat and coordinates)\n",
    "features = ['coastline', 'temp', 'dem', 'Urban', 'Park', 'Rural', 'Forest', 'Open_forest', 'Fjell', 'Open_area', 'Wetland', 'Open_bog', 'Freshwater', 'Marine', 'ykj_n', 'ykj_e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into present and absent data\n",
    "gdf_present = gdf[\n",
    "    (gdf['atlas_class_value'] == 'Varma pesintä') | \n",
    "    (gdf['atlas_class_value'] == 'Todennäköinen pesintä')\n",
    "]\n",
    "\n",
    "gdf_absent = gdf[\n",
    "    (gdf['atlas_class_value'] == 'Epätodennäköinen pesintä') | \n",
    "    (gdf['atlas_class_value'] == 'Mahdollinen pesintä')\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Number of present data:', len(gdf_present))\n",
    "print('Number of absent data:', len(gdf_absent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine presence and absence data into a single GeoDataFrame. Add column 'class'\n",
    "merged = helpers.stack_geodataframes(gdf_present, gdf_absent, add_class_label=True)\n",
    "merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YKJ grid square data with similar environmental features but without birds\n",
    "gdf_new_grids = gpd.read_file('YKJ10km_polygons/YKJ_polygons_corine_no_birds.gpkg')\n",
    "\n",
    "# Prepare the feature matrix for the new grid squares\n",
    "X_new_grids = gdf_new_grids[features]\n",
    "\n",
    "# Normalize feature columns\n",
    "X_new_grids = helpers.normalise_columns(X_new_grids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all columns in the merged GeoDataFrame\n",
    "merged = helpers.normalise_columns(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statistics(rf, y_test, rf_predictions, rf_probs):\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, rf_predictions)\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = precision_score(y_test, rf_predictions)\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "\n",
    "    # Calculate recall\n",
    "    recall = recall_score(y_test, rf_predictions)\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, rf_predictions)\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "    # Calculate ROC-AUC score\n",
    "    roc_auc = roc_auc_score(y_test, rf_probs)\n",
    "    print(f'ROC-AUC Score: {roc_auc:.4f}')\n",
    "\n",
    "    # Get feature importances\n",
    "    feature_importances = rf.feature_importances_\n",
    "\n",
    "    # Create a DataFrame to pair feature names with their importance scores\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': features, \n",
    "        'Importance': feature_importances\n",
    "    })\n",
    "\n",
    "    # Sort features by importance\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Display the top features\n",
    "    print(importance_df)\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Define cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by \"species_name\" and loop through each species\n",
    "for species_name, species_data in merged.groupby('species_name'):\n",
    "    if len(species_data) < 1000:\n",
    "        continue # Skip species with less than 1000 data points\n",
    "    \n",
    "    print('Species:', species_name)\n",
    "\n",
    "    # Prepare feature matrices and target variables\n",
    "    X = species_data[features]\n",
    "    y = species_data['class']\n",
    "    weights = species_data['weights']\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(X, y, weights, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Perform grid search with cross-validation\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train, sample_weight=weights_train)\n",
    "\n",
    "    # Get the best model from grid search\n",
    "    best_rf = grid_search.best_estimator_\n",
    "\n",
    "    rf_predictions = best_rf.predict(X_test)\n",
    "    rf_probs = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    #print_statistics(best_rf, y_test, rf_predictions, rf_probs)\n",
    "    \n",
    "    # Predict probabilities using the trained Random Forest model\n",
    "    gdf_new_grids['probability'] = best_rf.predict_proba(X_new_grids)[:, 1]\n",
    "\n",
    "    # Save the results to a new file\n",
    "    #gdf_new_grids.to_file(f'data_results/geopackages/{species_name}_predictions.gpkg', driver='GPKG')\n",
    "\n",
    "    # Plot the results on a map\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    gdf_new_grids.plot(column='probability', cmap='coolwarm', legend=True, ax=ax)\n",
    "    plt.title(f'{species_name} Probability Map')\n",
    "    #plt.show()\n",
    "\n",
    "    # Store results as png\n",
    "    plt.savefig(f'data_results/images/{species_name}_probability_map3.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
